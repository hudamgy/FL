{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R9vizijBRtj"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade tensorflow-federated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EJe8FE-BYbh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDQi3CE8Baj_"
      },
      "outputs": [],
      "source": [
        "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRtwlW6aBd0s"
      },
      "outputs": [],
      "source": [
        "def preprocess(dataset):\n",
        "  def format_fn(element):\n",
        "    return (tf.reshape(element['pixels'], [-1, 784]),\n",
        "            tf.reshape(element['label'], [-1, 1]))\n",
        "\n",
        "  return dataset.map(format_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptK1JKQKyLkv"
      },
      "outputs": [],
      "source": [
        "NUM_CLIENTS = 50\n",
        "NUM_CLASSES = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "posRRlWxUGi9"
      },
      "outputs": [],
      "source": [
        "def import_tff_mnist(n_samples, emnist_train):\n",
        "  client_datasets = []\n",
        "  for client_id in emnist_train.client_ids[:n_samples]:\n",
        "    client_dataset = emnist_train.create_tf_dataset_for_client(client_id)\n",
        "    client_datasets.extend(preprocess(client_dataset))\n",
        "  del(emnist_train)\n",
        "  return client_datasets\n",
        "\n",
        "client_datasets = import_tff_mnist(NUM_CLIENTS * 10, emnist_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BbupdsnIDxb"
      },
      "outputs": [],
      "source": [
        "non_iid_strength_factor = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laheW95hHzyS"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(client_datasets)\n",
        "\n",
        "# After we import the dataset, we are going to sort each (sample, label) by the label number in ascending order (from 0 to 9).\n",
        "client_datasets = sorted(client_datasets, key=lambda x: int(x[1]))\n",
        "\n",
        "# Then, we need to separate the samples by creating a dictionary of 10 different keys (0 -> 9) which are the labels.\n",
        "# Each key has a value which is a list of all samples belong to the label that the key represents.\n",
        "sorted_by_label = {str(k): [] for k in range(10)}\n",
        "for sample in client_datasets:\n",
        "  sorted_by_label[str(int(sample[1]))].append(sample)\n",
        "\n",
        "# This dictionary is going to hold the dataset separated by clients.\n",
        "# The dictionary will be filled in the for loop below where we distribute the samples between the clients.\n",
        "clients_datasets_dict = {k: [] for k in range(NUM_CLIENTS)}\n",
        "\n",
        "# The following dictionary contains 10 different keys (0 -> 9) which are the labels.\n",
        "# Each value is a list of size equals to NUM_CLIENTS.\n",
        "# Each list contains random picks along a range of \"label_list\" length (indices in ascending order). Some of theses indices will be dropped depening on the non_iid_strength_factor as we will see in the loop below.\n",
        "# This means that each client \"i\" will have chunk of samples from label_list[i-1] to label_list[i] for each label.\n",
        "lables_indices = {k: sorted(random.sample(range(len(label_list)), (NUM_CLIENTS - 1))) for label, label_list, k in zip(sorted_by_label.keys(), sorted_by_label.values(), range(NUM_CLASSES))}\n",
        "\n",
        "# The dictionary here is meant to track the next available label index for the next client.\n",
        "index_tracker = {label: 0 for label, _ in sorted_by_label.items()}\n",
        "\n",
        "# Starting the distributing process by first looping over the clients.\n",
        "for i in range(NUM_CLIENTS):\n",
        "  # client_execluded_classes holds the labels that the i-th client will be deprived from\n",
        "  client_execluded_classes = random.sample(range(NUM_CLASSES), non_iid_strength_factor)\n",
        "  for label, label_list in sorted_by_label.items():\n",
        "    # If the outer loop is not at the last client:\n",
        "    if i != (NUM_CLIENTS - 1):\n",
        "      # Pop the next index whatever.\n",
        "      label_index = lables_indices[int(label)].pop(0)\n",
        "      # If the i-th client can't have the current label in the inner loop, continue.\n",
        "      if int(label) in client_execluded_classes:\n",
        "        continue\n",
        "    # If the outer loop is at the last client, give me the last index.\n",
        "    else:\n",
        "      label_index = len(label_list)\n",
        "    # Assign the next chunck to the i-th client.\n",
        "    clients_datasets_dict[i].extend(label_list[index_tracker[label]:label_index])\n",
        "    random.shuffle(clients_datasets_dict[i])\n",
        "    # Keep track of the next index of the current label for the next client.\n",
        "    index_tracker[label] = label_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7bmel802w4_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import gaussian_kde\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert tensors to NumPy arrays and flatten them\n",
        "for i in random.sample(range(NUM_CLIENTS), k=5):\n",
        "  flat_arrays = [tensor[1].numpy().flatten() for tensor in clients_datasets_dict[i]]\n",
        "\n",
        "  # Concatenate the flattened arrays into a single array\n",
        "  concatenated_array = np.concatenate(flat_arrays)\n",
        "\n",
        "  # Estimate the kernel density\n",
        "  kde = gaussian_kde(concatenated_array)\n",
        "\n",
        "  # Generate a range of values for the x-axis\n",
        "  x_vals = np.linspace(concatenated_array.min(), concatenated_array.max(), num=1000)\n",
        "\n",
        "  # Evaluate the kernel density at each x value\n",
        "  y_vals = kde(x_vals)\n",
        "\n",
        "  # Plot the kernel density estimate\n",
        "  plt.plot(x_vals, y_vals, label=f'Client: {i}')\n",
        "\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Density')\n",
        "plt.title('KDE for Random Clients Data - 3 Labels Exclusion')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgeBSzil3Lgp"
      },
      "outputs": [],
      "source": [
        "dataset_size = sum([len(clients_datasets_dict[i]) for i in range(NUM_CLIENTS)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah1FfXbwBl_y"
      },
      "outputs": [],
      "source": [
        "central_emnist_test = emnist_test.create_tf_dataset_from_all_clients()\n",
        "central_emnist_test = preprocess(central_emnist_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IzA2_RHcs8S"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "NUM_CLIENTS = 50\n",
        "# C (clipping norm)\n",
        "clipping_norm = 50\n",
        "# ε (privacy budget)\n",
        "epsilon = 50\n",
        "# T (communication rounds)\n",
        "comms_round = 50\n",
        "# K clients for each round\n",
        "USERS_PER_ROUND = 20\n",
        "# δ here should be less than 1 over the total size of all datasets\n",
        "delta = (1 / dataset_size) * 0.5\n",
        "\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "current_epoch_num = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUAFEmHYid41"
      },
      "outputs": [],
      "source": [
        "c = math.sqrt(2.0 * math.log(1.25 / delta))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oVG19YvKieB7"
      },
      "source": [
        "constant $c \\geq \\sqrt{2 \\ln (1.25 / \\delta)}$ contributes to both global and local noise scales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zz5gzBDxcwnd"
      },
      "outputs": [],
      "source": [
        "def create_keras_model():\n",
        "  initializer = tf.keras.initializers.GlorotNormal(seed=0)\n",
        "  return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Input(shape=(784,)),\n",
        "      tf.keras.layers.Dense(256, activation='relu', kernel_initializer=initializer),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "\n",
        "input_spec = (tf.TensorSpec(shape=(None, 784), dtype=tf.float32),\n",
        "              tf.TensorSpec(shape=(None, 1), dtype=tf.int32))\n",
        "\n",
        "def model_fn():\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.models.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=input_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyesDnvIilWJ"
      },
      "outputs": [],
      "source": [
        "def get_global_stddev():\n",
        "  gamma = -1 * np.log(1 - (USERS_PER_ROUND / NUM_CLIENTS) + (USERS_PER_ROUND / NUM_CLIENTS) * np.exp((-1 * epsilon) / math.sqrt(USERS_PER_ROUND)))\n",
        "  if (current_epoch_num + 1) > (epsilon / gamma):\n",
        "    b = -1 * ((current_epoch_num + 1) / epsilon) * np.log(1 - (NUM_CLIENTS / USERS_PER_ROUND) + ((NUM_CLIENTS / USERS_PER_ROUND) * np.exp((-1 * epsilon) / (current_epoch_num + 1))))\n",
        "    stddev = (2 * c * clipping_norm * math.sqrt((((current_epoch_num + 1) ** 2) / (b ** 2)) - USERS_PER_ROUND)) / ((dataset_size / NUM_CLIENTS) * USERS_PER_ROUND * epsilon)\n",
        "  else:\n",
        "    stddev = 0\n",
        "  return stddev"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj8oitV7imil"
      },
      "source": [
        "The following formula corrsponds to the global noise standard deviation calculated in ``` get_global_stddev ```.\n",
        "\n",
        "\n",
        "\n",
        "$\\sigma_{\\mathrm{D}}= \\begin{cases}\\frac{2 c C \\sqrt{\\frac{T^2}{b^2}-L^2 K}}{m K \\epsilon} & T>\\frac{\\epsilon}{\\gamma} \\\\ 0 & T \\leq \\frac{\\epsilon}{\\gamma}\\end{cases}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQJVzwzjiuVP"
      },
      "outputs": [],
      "source": [
        "def clip_weights(weight, clipping_norm):\n",
        "  weight = tf.cast(weight, tf.float32)  # Cast the weight tensor to float32\n",
        "  max_value = tf.maximum(tf.constant(1.0, dtype=tf.float32), tf.divide(weight, tf.constant(clipping_norm, dtype=tf.float32)))\n",
        "  return weight.assign(weight / max_value)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lUuDEObBiv7W"
      },
      "source": [
        "The following is the clipping mechanism used at each client locally after completing each training epoch. The calculation is being done in `clip_weight` method.\n",
        "\n",
        "$w_i^{(t)}=w_i^{(t)} / \\max \\left(1, \\frac{\\left\\|w_i^{(t)}\\right\\|}{C}\\right)$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OIAel5kAi1ll"
      },
      "source": [
        "Each local noise scale is calculated as follows: $\\sigma_{\\mathrm{U}}=c L \\Delta s_{\\mathrm{U}} / \\epsilon$. The calculation is being done below in `client_update` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLp-SeL2c6R-"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def client_update(model, dataset, server_weights, client_optimizer):\n",
        "  \"\"\"Performs training (using the server model weights) on the client's dataset.\"\"\"\n",
        "  # Initialize the client model with the current server weights.\n",
        "\n",
        "  client_weights = model.trainable_variables\n",
        "  # Assign the server weights to the client model.\n",
        "  tf.nest.map_structure(lambda x, y: x.assign(y),\n",
        "                        client_weights, server_weights)\n",
        "\n",
        "  # Use the client_optimizer to update the local model.\n",
        "  for batch in dataset:\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Compute a forward pass on the batch of data\n",
        "      outputs = model.forward_pass(batch)\n",
        "\n",
        "    # Compute the corresponding gradient\n",
        "    grads = tape.gradient(outputs.loss, client_weights)\n",
        "    grads_and_vars = zip(grads, client_weights)\n",
        "\n",
        "    # Apply the gradient using a client optimizer.\n",
        "    client_optimizer.apply_gradients(grads_and_vars)\n",
        "\n",
        "  client_weights = tf.nest.map_structure(lambda x: x.assign(clip_weights(x, clipping_norm)), client_weights)\n",
        "  sensitivity = 2 * (clipping_norm) / (dataset_size / NUM_CLIENTS)\n",
        "  stddev = (c * sensitivity * (current_epoch_num + 1)) / epsilon\n",
        "  for var in client_weights:\n",
        "    stddev = tf.cast(stddev, var.dtype)\n",
        "    noise = tf.random.normal(shape=tf.shape(var), mean=0.0, stddev=stddev)\n",
        "    var.assign_add(noise)\n",
        "  return client_weights\n",
        "\n",
        "@tf.function\n",
        "def server_update(model, mean_client_weights):\n",
        "  \"\"\"Updates the server model weights as the average of the client model weights.\"\"\"\n",
        "  model_weights = model.trainable_variables\n",
        "  # Assign the mean client weights to the server model.\n",
        "  tf.nest.map_structure(lambda x, y: x.assign(y),\n",
        "                        model_weights, mean_client_weights)\n",
        "  stddev = get_global_stddev()\n",
        "  for var in model_weights:\n",
        "    stddev = tf.cast(stddev, var.dtype)\n",
        "    noise = tf.random.normal(shape=tf.shape(var), mean=0.0, stddev=stddev)\n",
        "    var.assign_add(noise)\n",
        "  return model_weights\n",
        "\n",
        "@tff.tf_computation\n",
        "def server_init():\n",
        "  model = model_fn()\n",
        "  return model.trainable_variables\n",
        "\n",
        "@tff.federated_computation\n",
        "def initialize_fn():\n",
        "  return tff.federated_value(server_init(), tff.SERVER)\n",
        "\n",
        "whimsy_model = model_fn()\n",
        "tf_dataset_type = tff.SequenceType(whimsy_model.input_spec)\n",
        "model_weights_type = server_init.type_signature.result\n",
        "\n",
        "@tff.tf_computation(tf_dataset_type, model_weights_type)\n",
        "def client_update_fn(tf_dataset, server_weights):\n",
        "  model = model_fn()\n",
        "  client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "  return client_update(model, tf_dataset, server_weights, client_optimizer)\n",
        "\n",
        "@tff.tf_computation(model_weights_type)\n",
        "def server_update_fn(mean_client_weights):\n",
        "  model = model_fn()\n",
        "  return server_update(model, mean_client_weights)\n",
        "\n",
        "federated_server_type = tff.FederatedType(model_weights_type, tff.SERVER)\n",
        "federated_dataset_type = tff.FederatedType(tf_dataset_type, tff.CLIENTS)\n",
        "\n",
        "@tff.federated_computation(federated_server_type, federated_dataset_type)\n",
        "def next_fn(server_weights, federated_dataset):\n",
        "  # Broadcast the server weights to the clients.\n",
        "  server_weights_at_client = tff.federated_broadcast(server_weights)\n",
        "\n",
        "  # Each client computes their updated weights.\n",
        "  client_weights = tff.federated_map(\n",
        "      client_update_fn, (federated_dataset, server_weights_at_client))\n",
        "\n",
        "  # The server averages these updates.\n",
        "  mean_client_weights = tff.federated_mean(client_weights)\n",
        "\n",
        "  # The server updates its model.\n",
        "  server_weights = tff.federated_map(server_update_fn, mean_client_weights)\n",
        "\n",
        "  return server_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJR3gVdyI3MD"
      },
      "outputs": [],
      "source": [
        "federated_algorithm = tff.templates.IterativeProcess(\n",
        "    initialize_fn=initialize_fn,\n",
        "    next_fn=next_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xeo2UdzdIcp"
      },
      "outputs": [],
      "source": [
        "def evaluate(server_state):\n",
        "  keras_model = create_keras_model()\n",
        "  keras_model.compile(\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "  keras_model.set_weights(server_state)\n",
        "  hst = keras_model.evaluate(central_emnist_test)\n",
        "  return hst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1lFUe7HjEab"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if len(device_name) > 0:\n",
        "    print(\"Found GPU at: {}\".format(device_name))\n",
        "else:\n",
        "    device_name = \"/device:CPU:0\"\n",
        "    print(\"No GPU, using {}.\".format(device_name))\n",
        "\n",
        "with tf.device(device_name):\n",
        "  server_state = federated_algorithm.initialize()\n",
        "\n",
        "history = {\n",
        "    'loss': [],\n",
        "    'accuracy': []\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ergdd-la73Y2"
      },
      "outputs": [],
      "source": [
        "clients_ids = [i for i in range(50)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MWjGjEWBJ1Xc"
      },
      "outputs": [],
      "source": [
        "for round in range(comms_round):\n",
        "  current_epoch_num = round + 1\n",
        "  print(f'Round: {current_epoch_num}')\n",
        "  selected_clients = random.sample(clients_ids, USERS_PER_ROUND)\n",
        "  current_federated_train_data = [clients_datasets_dict[client_id] for client_id in selected_clients]\n",
        "  server_state = federated_algorithm.next(server_state, current_federated_train_data)\n",
        "  loss, accuracy = evaluate(server_state)\n",
        "  history['loss'].append(loss)\n",
        "  history['accuracy'].append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtvizYqQIHg2"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(f'history_c{clipping_norm}_e{epsilon}_k{USERS_PER_ROUND}_sf{non_iid_strength_factor}.pickle', 'wb') as file:\n",
        "  pickle.dump(history, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlBjm05SI3h-"
      },
      "outputs": [],
      "source": [
        "plt.plot(history['loss'])\n",
        "plt.title('Evaluation')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(['loss'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kY_m_U-I194"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting Loss and Accuracy on top of each other\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
        "fig.suptitle('Model Performance', fontsize=14)\n",
        "\n",
        "# Plot Loss\n",
        "ax1.plot(history['loss'])\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.legend(['Loss'], loc='upper right')\n",
        "\n",
        "# Plot Accuracy\n",
        "ax2.plot(history['accuracy'])\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.legend(['Accuracy'], loc='lower right')\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8ctyqvTLGCN"
      },
      "outputs": [],
      "source": [
        "model = create_keras_model()\n",
        "model.set_weights(server_state)\n",
        "model.save_weights(f'/content/FLDP_non_iid_c{clipping_norm}_e{epsilon}_k{USERS_PER_ROUND}_sf{non_iid_strength_factor}.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
