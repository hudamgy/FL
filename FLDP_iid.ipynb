{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd1KXsFqolt0"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade tensorflow-federated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wt3LhjzxoyKq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3No3JjFsK4L"
      },
      "outputs": [],
      "source": [
        "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fZmxs4tRoztZ"
      },
      "outputs": [],
      "source": [
        "def preprocess(dataset):\n",
        "  def format_fn(element):\n",
        "    return (tf.reshape(element['pixels'], [-1, 784]),\n",
        "            tf.reshape(element['label'], [-1, 1]))\n",
        "\n",
        "  return dataset.map(format_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qFDa1X5EBs3e"
      },
      "outputs": [],
      "source": [
        "NUM_CLIENTS = 50\n",
        "NUM_CLASSES = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CeiCijbUBgaP"
      },
      "outputs": [],
      "source": [
        "client_ids = sorted(emnist_train.client_ids[:NUM_CLIENTS * 10])\n",
        "federated_train_data = [preprocess(emnist_train.create_tf_dataset_for_client(x))\n",
        "  for x in client_ids\n",
        "]\n",
        "del(emnist_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M6X7tg-jRg3U"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "combined_dataset = functools.reduce(tf.data.Dataset.concatenate, federated_train_data)\n",
        "combined_dataset = list(combined_dataset)\n",
        "random.shuffle(combined_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_size = len(combined_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9kEHaJ9yYMdn"
      },
      "outputs": [],
      "source": [
        "# The distribution here is simple.\n",
        "# All clients will equally have the same number of samples which is dataset_size divided by the number of total clients.\n",
        "def create_clients_datasets(dataset):\n",
        "  clients_datasets = []\n",
        "  lower_bound = 0\n",
        "  upper_bound = dataset_size // NUM_CLIENTS\n",
        "  for i in range(NUM_CLIENTS):\n",
        "    clients_datasets.append(dataset[lower_bound:upper_bound])\n",
        "    lower_bound = upper_bound\n",
        "    upper_bound += dataset_size // NUM_CLIENTS\n",
        "  return clients_datasets\n",
        "\n",
        "clients_datasets = create_clients_datasets(combined_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVjWXV9hWB22"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import gaussian_kde\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert tensors to NumPy arrays and flatten them\n",
        "for i in range(NUM_CLIENTS):\n",
        "  flat_arrays = [tensor[1].numpy().flatten() for tensor in clients_datasets[i]]\n",
        "\n",
        "  # Concatenate the flattened arrays into a single array\n",
        "  concatenated_array = np.concatenate(flat_arrays)\n",
        "\n",
        "  # Estimate the kernel density\n",
        "  kde = gaussian_kde(concatenated_array)\n",
        "\n",
        "  # Generate a range of values for the x-axis\n",
        "  x_vals = np.linspace(concatenated_array.min(), concatenated_array.max(), num=1000)\n",
        "\n",
        "  # Evaluate the kernel density at each x value\n",
        "  y_vals = kde(x_vals)\n",
        "\n",
        "  # Plot the kernel density estimate\n",
        "  plt.plot(x_vals, y_vals)\n",
        "\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Kernel Density Estimate for 50 Clients')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HI6PF8cjhwfX"
      },
      "outputs": [],
      "source": [
        "central_emnist_test = emnist_test.create_tf_dataset_from_all_clients()\n",
        "central_emnist_test = preprocess(central_emnist_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VFVpOUFSr0G5"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "NUM_CLIENTS = 50\n",
        "# C (clipping norm)\n",
        "clipping_norm = 50\n",
        "# ε (privacy budget)\n",
        "epsilon = 50\n",
        "# T (communication rounds)\n",
        "comms_round = 50\n",
        "# K clients for each round\n",
        "USERS_PER_ROUND = 20\n",
        "# δ here should be less than 1 over the total size of all datasets\n",
        "delta = (1 / dataset_size) * 0.5\n",
        "\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "current_epoch_num = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtFYCiGiLRws"
      },
      "outputs": [],
      "source": [
        "c = math.sqrt(2.0 * math.log(1.25 / delta))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k4-gGfm2LSZN"
      },
      "source": [
        "constant $c \\geq \\sqrt{2 \\ln (1.25 / \\delta)}$ contributes to both global and local noise scales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3jUN0VD0ft9A"
      },
      "outputs": [],
      "source": [
        "def create_keras_model():\n",
        "  initializer = tf.keras.initializers.GlorotNormal(seed=0)\n",
        "  return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Input(shape=(784,)),\n",
        "      tf.keras.layers.Dense(256, activation='relu', kernel_initializer=initializer),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "\n",
        "input_spec = (tf.TensorSpec(shape=(None, 784), dtype=tf.float32),\n",
        "              tf.TensorSpec(shape=(None, 1), dtype=tf.int32))\n",
        "\n",
        "def model_fn():\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.models.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=input_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NESaxXvNL67J"
      },
      "outputs": [],
      "source": [
        "def get_global_stddev():\n",
        "  gamma = -1 * np.log(1 - (USERS_PER_ROUND / NUM_CLIENTS) + (USERS_PER_ROUND / NUM_CLIENTS) * np.exp((-1 * epsilon) / math.sqrt(USERS_PER_ROUND)))\n",
        "  if (current_epoch_num + 1) > (epsilon / gamma):\n",
        "    b = -1 * ((current_epoch_num + 1) / epsilon) * np.log(1 - (NUM_CLIENTS / USERS_PER_ROUND) + ((NUM_CLIENTS / USERS_PER_ROUND) * np.exp((-1 * epsilon) / (current_epoch_num + 1))))\n",
        "    stddev = (2 * c * clipping_norm * math.sqrt((((current_epoch_num + 1) ** 2) / (b ** 2)) - USERS_PER_ROUND)) / ((dataset_size / NUM_CLIENTS) * USERS_PER_ROUND * epsilon)\n",
        "  else:\n",
        "    stddev = 0\n",
        "  return stddev"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YiBCX902L7gB"
      },
      "source": [
        "The following formula corrsponds to the global noise standard deviation calculated in ``` get_global_stddev ```.\n",
        "\n",
        "\n",
        "\n",
        "$\\sigma_{\\mathrm{D}}= \\begin{cases}\\frac{2 c C \\sqrt{\\frac{T^2}{b^2}-L^2 K}}{m K \\epsilon} & T>\\frac{\\epsilon}{\\gamma} \\\\ 0 & T \\leq \\frac{\\epsilon}{\\gamma}\\end{cases}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOI_-qwHL_IP"
      },
      "outputs": [],
      "source": [
        "def clip_weight(weight, clipping_norm):\n",
        "  weight = tf.cast(weight, tf.float32)  # Cast the weight tensor to float32\n",
        "  max_value = tf.maximum(tf.constant(1.0, dtype=tf.float32), tf.divide(weight, tf.constant(clipping_norm, dtype=tf.float32)))\n",
        "  return weight.assign(tf.divide(weight, max_value))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4YJzQdctMCwN"
      },
      "source": [
        "The following is the clipping mechanism used at each client locally after completing each training epoch. The calculation is being done in `clip_weight` method.\n",
        "\n",
        "$w_i^{(t)}=w_i^{(t)} / \\max \\left(1, \\frac{\\left\\|w_i^{(t)}\\right\\|}{C}\\right)$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l48zjnCsMIhf"
      },
      "source": [
        "Each local noise scale is calculated as follows: $\\sigma_{\\mathrm{U}}=c L \\Delta s_{\\mathrm{U}} / \\epsilon$. The calculation is being done below in `client_update` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "elHSuEcPpI9R"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def client_update(model, dataset, server_weights, client_optimizer):\n",
        "  \"\"\"Performs training (using the server model weights) on the client's dataset.\"\"\"\n",
        "  # Initialize the client model with the current server weights.\n",
        "\n",
        "  client_weights = model.trainable_variables\n",
        "  # Assign the server weights to the client model.\n",
        "  tf.nest.map_structure(lambda x, y: x.assign(y),\n",
        "                        client_weights, server_weights)\n",
        "\n",
        "  # Use the client_optimizer to update the local model.\n",
        "  for batch in dataset:\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Compute a forward pass on the batch of data\n",
        "      outputs = model.forward_pass(batch)\n",
        "\n",
        "    # Compute the corresponding gradient\n",
        "    grads = tape.gradient(outputs.loss, client_weights)\n",
        "    grads_and_vars = zip(grads, client_weights)\n",
        "\n",
        "    # Apply the gradient using a client optimizer.\n",
        "    client_optimizer.apply_gradients(grads_and_vars)\n",
        "\n",
        "  client_weights = tf.nest.map_structure(lambda x: x.assign(clip_weight(x, clipping_norm)), client_weights)\n",
        "  sensitivity = 2 * (clipping_norm) / (dataset_size / len(client_ids))\n",
        "  stddev = (c * sensitivity * (current_epoch_num + 1)) / epsilon\n",
        "  for var in client_weights:\n",
        "    stddev = tf.cast(stddev, var.dtype)\n",
        "    noise = tf.random.normal(shape=tf.shape(var), mean=0.0, stddev=stddev)\n",
        "    var.assign_add(noise)\n",
        "  return client_weights\n",
        "\n",
        "@tf.function\n",
        "def server_update(model, mean_client_weights):\n",
        "  \"\"\"Updates the server model weights as the average of the client model weights.\"\"\"\n",
        "  model_weights = model.trainable_variables\n",
        "  # Assign the mean client weights to the server model.\n",
        "  tf.nest.map_structure(lambda x, y: x.assign(y),\n",
        "                        model_weights, mean_client_weights)\n",
        "  stddev = get_global_stddev()\n",
        "  for var in model_weights:\n",
        "    stddev = tf.cast(stddev, var.dtype)\n",
        "    noise = tf.random.normal(shape=tf.shape(var), mean=0.0, stddev=stddev)\n",
        "    var.assign_add(noise)\n",
        "  return model_weights\n",
        "\n",
        "@tff.tf_computation\n",
        "def server_init():\n",
        "  model = model_fn()\n",
        "  return model.trainable_variables\n",
        "\n",
        "@tff.federated_computation\n",
        "def initialize_fn():\n",
        "  return tff.federated_value(server_init(), tff.SERVER)\n",
        "\n",
        "whimsy_model = model_fn()\n",
        "tf_dataset_type = tff.SequenceType(whimsy_model.input_spec)\n",
        "model_weights_type = server_init.type_signature.result\n",
        "\n",
        "\n",
        "@tff.tf_computation(tf_dataset_type, model_weights_type)\n",
        "def client_update_fn(tf_dataset, server_weights):\n",
        "  model = model_fn()\n",
        "  client_optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
        "  return client_update(model, tf_dataset, server_weights, client_optimizer)\n",
        "\n",
        "@tff.tf_computation(model_weights_type)\n",
        "def server_update_fn(mean_client_weights):\n",
        "  model = model_fn()\n",
        "  return server_update(model, mean_client_weights)\n",
        "\n",
        "federated_server_type = tff.FederatedType(model_weights_type, tff.SERVER)\n",
        "federated_dataset_type = tff.FederatedType(tf_dataset_type, tff.CLIENTS)\n",
        "\n",
        "@tff.federated_computation(federated_server_type, federated_dataset_type)\n",
        "def next_fn(server_weights, federated_dataset):\n",
        "  # Broadcast the server weights to the clients.\n",
        "  server_weights_at_client = tff.federated_broadcast(server_weights)\n",
        "\n",
        "  # Each client computes their updated weights.\n",
        "  client_weights = tff.federated_map(\n",
        "      client_update_fn, (federated_dataset, server_weights_at_client))\n",
        "\n",
        "  # The server averages these updates.\n",
        "  mean_client_weights = tff.federated_mean(client_weights)\n",
        "\n",
        "  # The server updates its model.\n",
        "  server_weights = tff.federated_map(server_update_fn, mean_client_weights)\n",
        "\n",
        "  return server_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmlkoYk-MPHg"
      },
      "outputs": [],
      "source": [
        "federated_algorithm = tff.templates.IterativeProcess(\n",
        "    initialize_fn=initialize_fn,\n",
        "    next_fn=next_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "b8fU4TnkdaGG"
      },
      "outputs": [],
      "source": [
        "def evaluate(server_state):\n",
        "  keras_model = create_keras_model()\n",
        "  keras_model.compile(\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "  keras_model.set_weights(server_state)\n",
        "  hst = keras_model.evaluate(central_emnist_test)\n",
        "  return hst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oit1sA6qBFq0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if len(device_name) > 0:\n",
        "    print(\"Found GPU at: {}\".format(device_name))\n",
        "else:\n",
        "    device_name = \"/device:CPU:0\"\n",
        "    print(\"No GPU, using {}.\".format(device_name))\n",
        "\n",
        "with tf.device(device_name):\n",
        "  server_state = federated_algorithm.initialize()\n",
        "\n",
        "history = {\n",
        "    'loss': [],\n",
        "    'accuracy': []\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ggkC6fxo6O7x"
      },
      "outputs": [],
      "source": [
        "clients_ids = [i for i in range(50)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAS4SZS_qH8v"
      },
      "outputs": [],
      "source": [
        "for round in range(comms_round):\n",
        "  current_epoch_num = round + 1\n",
        "  print(f'Round: {current_epoch_num}')\n",
        "  selected_clients = random.sample(clients_ids, USERS_PER_ROUND)\n",
        "  current_federated_train_data = [clients_datasets[client_id] for client_id in selected_clients]\n",
        "  server_state = federated_algorithm.next(server_state, current_federated_train_data)\n",
        "  loss, accuracy = evaluate(server_state)\n",
        "  history['loss'].append(loss)\n",
        "  history['accuracy'].append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWmM2apBCZyQ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(f'history_c{clipping_norm}_e{epsilon}_k{USERS_PER_ROUND}.pickle', 'wb') as file:\n",
        "  pickle.dump(history, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ0IWBmWfIp6"
      },
      "outputs": [],
      "source": [
        "plt.plot(history['loss'])\n",
        "plt.title('Evaluation')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(['loss'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFJ5ywaKA_OR"
      },
      "outputs": [],
      "source": [
        "# Plotting Loss and Accuracy on top of each other\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
        "fig.suptitle('Model Performance', fontsize=14)\n",
        "\n",
        "# Plot Loss\n",
        "ax1.plot(history['loss'])\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.legend(['Loss'], loc='upper right')\n",
        "\n",
        "# Plot Accuracy\n",
        "ax2.plot(history['accuracy'])\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.legend(['Accuracy'], loc='lower right')\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDvhhTQTDy2c"
      },
      "outputs": [],
      "source": [
        "model = create_keras_model()\n",
        "model.set_weights(server_state)\n",
        "model.save_weights(f'/content/FLDP_iid_c{clipping_norm}_e{epsilon}_k{USERS_PER_ROUND}.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
